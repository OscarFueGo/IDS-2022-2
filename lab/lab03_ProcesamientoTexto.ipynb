{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bhH0v_iiFcmT"},"source":["![](https://drive.google.com/uc?export=view&id=1-5X9OUkA-C2Ih1gOS9Jd7GmkTWUEpDg1)\n","\n","# Laboratorio 03: Procesamiento de lenguaje natural con Python\n","## Introducción a _Data Science_\n","\n","<!--<center>\n","    <img src='images/GAN.jpeg'style=\"width: 600px;\">\n","</center>-->\n","\n","**Profesor**: Juan Bekios Calfa\n","\n","**Carreras**: ICCI, ITI, ICI\n","\n","<!--<sub><sup>Tutorial: GANS. Sensio Artificial Intelligence [link](https://sensioai.com/blog/051_gans)</sup></sub> -->"]},{"cell_type":"markdown","metadata":{"id":"Y7RYtLBtqplg"},"source":["\n","\n","---\n","**Nombre del alumno**: --Indicar nombre completo--\n","\n","**Carrera**: -- Indicar carrera --\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"gYJvkLk0qk5L"},"source":["# 1. Introducción\n","\n","El laboratorio se centra en el tratamiento de **datos de texto libre**.  Además, de los **datos estructurado**s y de los **grafos** que hemos analizado anteriormente, el **texto libre** constituye uno de los tipos más comunes de datos \"ampliamente disponibles\": \n","\n","* Las páginas web\n","* Los campos de \"comentarios\" no estructurados de muchas bases de datos relacionales \n","* Otras fuentes de datos de gran tamaño fáciles de obtener vienen naturalmente en forma de texto libre.  \n","\n","La diferencia notable, por supuesto, es que, a diferencia de los tipos de datos que hemos analizado antes, el texto libre carece de la estructura \"fácilmente extraíble\" inherente a los tipos de datos anteriores que hemos considerado.\n","\n","El laboratorio tiene como objetivo conocer algunas técnicas para pre-procesar texto para ser utilizado como entrada a un clasificador o regresor.\n","\n","Se estudiarán diferentes librerías que permiten preparar el texto para luego codificarlo en una estructura de datos manejable."]},{"cell_type":"markdown","metadata":{"id":"PC5khnE6tBA8"},"source":["## 1.5 Visualización de palabras (TF)\n","\n","El modelo de bolsa de palabras (*Bag of words*) es, con mucho, el medio más común de representar documentos en la **ciencia de datos**. Según este modelo, un documento se describe únicamente por el conjunto de palabras (y posiblemente su número) que lo componen. Se ignora toda la información sobre el orden real de las palabras. Se trata esencialmente de la llamada \"nube de palabras\" de un documento."]},{"cell_type":"code","metadata":{"id":"LRa2dPSRtDIj"},"source":["from bs4 import BeautifulSoup\n","import requests\n","import re\n","\n","response = requests.get(\"http://www.datasciencecourse.org\")\n","root = BeautifulSoup(response.content, \"lxml\")\n","\n","from wordcloud import WordCloud\n","wc = WordCloud(width=800,height=400).generate(re.sub(r\"\\s+\",\" \", root.text))\n","wc.to_image()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install wikipedia"],"metadata":{"id":"UxmR3Ooc6tD7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import wikipedia\n","\n","wikipedia.set_lang(\"es\")\n","results_list = wikipedia.search(\"Barack\")\n","\n","for each_result in results_list:\n","  print(each_result)"],"metadata":{"id":"IeMPw7wZ6jgs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Extraemos dos textos de la base de datos"],"metadata":{"id":"TCagsPRwDvHf"}},{"cell_type":"code","source":["bo01 = wikipedia.page(\"Barack Obama\")\n","bo01.content"],"metadata":{"id":"tyYk3cGz8s-f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bo02 = wikipedia.page(\"Moisés Barack\")\n","bo02.content"],"metadata":{"id":"2EQ7jBFrDzP1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wc = WordCloud(width=800,height=400).generate(re.sub(r\"\\s+\",\" \", bo01.content))\n","wc.to_image()"],"metadata":{"id":"95TYXz7D8UIj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"504mEOSthOGX"},"source":["# NLTK\n","\n","## Instalar librerías"]},{"cell_type":"code","metadata":{"id":"WU7WAoa2fSp_"},"source":["import nltk\n","nltk.download('popular')"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Obtener TOKENs del texto\n","tokens = nltk.word_tokenize(bo01.content)\n","print(tokens)"],"metadata":{"id":"MOg3FbXX_mMd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MKIDsPHfYVj4"},"source":["## Algunos comandos\n","\n","### Eliminar signos de puntuación en el texto"]},{"cell_type":"code","metadata":{"id":"G8qs88eOhGL5"},"source":["# Eliminar espacios \n","nueva_linea = re.sub(r\"\\s+\",\" \", bo01.content)\n","tokens = nltk.word_tokenize(nueva_linea)\n","print(tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4pfHDMRk6QZJ"},"source":["from nltk.corpus import stopwords\n","\n","# Palabras innecesarias\n","sw = stopwords.words('spanish')\n","\n","filtered_sentence = []\n","for w in tokens: \n","    if w not in sw: \n","        filtered_sentence.append(w)\n","\n","print(filtered_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wc = WordCloud(width=800,height=400).generate(\" \".join(filtered_sentence))\n","wc.to_image()"],"metadata":{"id":"6-3LbJhZ_7-O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L7fOiB6XYkWH"},"source":["### Configurar el corpus a español y crearlo"]},{"cell_type":"markdown","metadata":{"id":"tvSGgxxD1PVP"},"source":["## Obtener los vectores de frecuencia (TF, *term frequency matrix*) \n","\n","---\n","\n","En este caso, podemos representar los documentos mediante una matriz de frecuencias de términos de $m \\times n$, donde $m$ indica el número de documentos y $n$ el tamaño del vocabulario (es decir, el número de palabras únicas en todos los documentos).  Para ver (de forma ingenua) cómo construir esta lista, consideremos primero una forma sencilla de obtener una lista de todas las palabras únicas en todos los documentos.  En general, no es necesario ordenar la lista de palabras, pero lo haremos por simplicidad.  Es una buena idea generar también un diccionario que asigne las palabras a su índice en esta lista, ya que con frecuencia querremos buscar el índice correspondiente a una palabra."]},{"cell_type":"code","source":["vocab_dict = {k:i for i,k in enumerate(filtered_sentence)}\n","print(filtered_sentence, \"\\n\")\n","print(vocab_dict, \"\\n\")"],"metadata":{"id":"wZB-zSv6Bkfp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Vectores de frecuencia utilizando sklearn\n","\n","Listaremos las palabras que no tienen relevancia en un texto y es preferible eliminarlas (*stop words*)."],"metadata":{"id":"ifDL5JbCCoX7"}},{"cell_type":"code","metadata":{"id":"a5wrbl5Z1H0P"},"source":["# Librerías sklearn - Machine Learning\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.corpus import stopwords\n","\n","sw = stopwords.words('spanish')\n","print(sw)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Calculamos el vector TF."],"metadata":{"id":"9AgbgBeWDGay"}},{"cell_type":"code","metadata":{"id":"YAq5NlrQ6ADM"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","sw = stopwords.words('spanish')\n","# print(sw)\n","vectorizer = CountVectorizer(stop_words=sw)\n","\n","freq_matrix = vectorizer.fit_transform([bo01.content, bo02.content])\n","#print(freq_matrix)\n","\n","feature_names = vectorizer.get_feature_names()\n","dense_frec = freq_matrix.todense()\n","denselist_frec = dense_frec.tolist()\n","df = pd.DataFrame(denselist_frec, columns=feature_names)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Calculamos el vector TF eliminando las palabras que contengan caracteres que no sean alfanuméricos."],"metadata":{"id":"cJ0bZq3vDU9x"}},{"cell_type":"code","metadata":{"id":"i2qq4WYUOZZZ"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","sw = stopwords.words('spanish')\n","# print(sw)\n","vectorizer = CountVectorizer(stop_words=sw, token_pattern=r'[^\\d\\W]+')\n","\n","freq_matrix = vectorizer.fit_transform([bo01.content, bo02.content])\n","# print(freq_matrix)\n","feature_names = vectorizer.get_feature_names()\n","\n","dense_frec = freq_matrix.todense()\n","denselist_frec = dense_frec.tolist()\n","df = pd.DataFrame(denselist_frec, columns=feature_names)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OhboWYMDAuIC"},"source":["## Obtener frecuencia inversa entre documentos (idf)\n","\n","### Frecuencia inversa del documento \n","\n","Un problema obvio con el uso de recuentos de frecuencia de términos normales para representar un documento es que el vector del documento (y las similitudes resultantes que consideraremos) a menudo estará \"dominado\" por palabras muy comunes, por ejemplo \"de\", \"el\", \"es\", en los documentos de ejemplo anteriores.  Este problema puede mitigarse hasta cierto punto excluyendo las llamadas \"stop words\" (palabras comunes en inglés como \"the\", \"a\", \"of\" que no se consideran relevantes para los documentos concretos) de la matriz de frecuencia de términos.  Pero esto sigue ignorando el caso en el que una palabra que puede no ser una palabra de parada genérica sigue apareciendo en un gran número de documentos.  Intuitivamente, esperamos que las palabras más \"importantes\" de un documento sean precisamente las que sólo aparecen en un número relativamente pequeño de documentos, por lo que queremos descontar el peso de los términos que aparecen con mucha frecuencia.\n","\n","Esto puede lograrse mediante el peso de la frecuencia inversa de los documentos para las palabras.  Al igual que con las frecuencias de términos, existen diferentes ponderaciones de este término, pero la formulación más común es\n","\n","\\begin{equation}\n","\\mathrm{idf}_j = \\log\\left(\\frac{\\mbox{# documents}}{\\mbox{# documents with word $j$}}\\right).\n","\\end{equation}\n","\n","Por ejemplo, si la palabra aparece en todos los documentos, el peso de la frecuencia inversa del documento será cero (logaritmo de uno).  Por el contrario, si una palabra sólo aparece en un documento, su frecuencia documental inversa será $\\log (\\mbox{# documentos})$.\n","\n","Tenga en cuenta que la frecuencia inversa de documentos es un término _por palabra_, a diferencia de la frecuencia de términos, que es _por palabra y documento_.  Podemos calcular la frecuencia inversa de documentos para nuestro conjunto de datos de la siguiente manera, que principalmente sólo requiere contar cuántos documentos contienen cada palabra."]},{"cell_type":"code","metadata":{"id":"Ev2Lw3QSAu_h"},"source":["from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","\n","transformer = TfidfTransformer()\n","tfidf_matrix = transformer.fit_transform(freq_matrix)\n","\n","dense = tfidf_matrix.todense()\n","denselist = dense.tolist()\n","df = pd.DataFrame(denselist, columns=feature_names)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"afpFeUOSEPNf"},"source":["## Obtener frecuencia inversa entre documentos (idf) de forma directa"]},{"cell_type":"code","metadata":{"id":"FYm-d_Qb1QH3"},"source":["vectorizer = TfidfVectorizer(stop_words=sw, token_pattern=r'[^\\d\\W]+')\n","vectors = vectorizer.fit_transform([bo01.content, bo02.content])\n","# print(vectors)\n","feature_names = vectorizer.get_feature_names()\n","# print(feature_names)\n","dense = vectors.todense()\n","denselist = dense.tolist()\n","df = pd.DataFrame(denselist, columns=feature_names)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NB8M6gP7ZKnc"},"source":["# Tarea\n","\n","1. Obtener los 100 primeros documentos de Wikipedia y crear un corpus.\n","2. Obtener 5 documentos no utilizados para entrenar el corpus y generar:\n","\n","    1. Vectores tf.\n","    2. Vectores idf a partir del vector tf.\n","    3. Vector idf de manera directa.\n","    4. **Explicar los resultados.**. ¿Qué problemas observó con las difentes técnicas utilizadas?\n","    5. Comparar la semejanza de todos los documentos utilizando TF. ¿Cual es el más parecido?. \n","    6. Comparar la semejanza de todos los documentos utilizando TF-IDF. ¿Cual es el más parecido?.\n","    7. A su juicio, ¿cual de las dos codificaciones de texto es mejor?. **Explicar los resultados.**\n","    \n","    Calcule la matriz de semejanza para una mejor explicación. Utilice el siguiente ejemplo:\n","        \n","\n","|           \t| $Documento_3$ \t|  $Documento_4$\t | $Documento_5$ | $Documento_6$| $Documento_7$ \t |\n","|-----------\t|:-------------:|:------------:|:-------------:|:-------------:|:------------:|\n","| $Documento_3$ |    1.00  \t  |    0.00  \t |    0.00  \t  |    0.00  \t |    0.00  \t  |\n","| $Documento_4$ |    0.00  \t  |    1.00  \t |    0.00  \t  |    0.00  \t |    0.00  \t  |\n","| $Documento_5$ |    0.00  \t  |    0.00  \t |    1.00  \t  |    0.00  \t |    0.00  \t  |\n","| $Documento_6$ |    0.00  \t  |    0.00  \t |    0.00  \t  |    1.00  \t |    0.00  \t  |\n","| $Documento_7$ |    0.00  \t  |    0.00  \t |    0.00  \t  |    0.00  \t |    1.00  \t  |\n","\n","**Fecha de entrega**: Jueves 6 de mayo."]},{"cell_type":"markdown","metadata":{"id":"X3H42atWXdnG"},"source":["# I. Anexo: Similitud del coseno\n","\n","Dada una matriz TF-IDF (o simplemente frecuencia de término), una de las preguntas más comunes a abordar es calcular la similitud entre varios documentos en el _corpus_.  Una medida común (métrica) para hacerlo es calcular la similitud del coseno entre dos documentos diferentes. Esto es simplemente un producto interno normalizado entre los vectores que describen cada documento. Específicamente,\n","\n","\\begin{equation}\n","\\mbox{SimilitudCoseno}(x,y) = \\frac{x^T y}{\\|x\\|_2 \\cdot \\|y\\|_2}.\n","\\end{equation}\n","\n","La **similitud del coseno** es un número entre cero (lo que significa que los dos documentos no comparten términos en común) y uno (lo que significa que los dos documentos tienen exactamente la misma frecuencia de términos o representación TFIDF). De hecho, la similitud del coseno es exactamente la inversa de la distancia Eucliean al cuadrado entre los vectores de documentos normalizados; formalmente, para $\\tilde{x} = x / \\|x\\|_2$ and $\\tilde{y} = y / \\|y\\|_2$,\n","\n","\\begin{equation}\n","\\begin{split}\n","\\frac{1}{2}\\|\\tilde{x} - \\tilde{y}\\|_2^2 & = \\frac{1}{2}(\\tilde{x} - \\tilde{y})^T (\\tilde{x} - \\tilde{y}) \\\\\n","& = \\frac{1}{2} (\\tilde{x}^T \\tilde{x} - 2 \\tilde{x}^T \\tilde{y} + \\tilde{y}^T \\tilde{y}) \\\\\n","& = \\frac{1}{2} (1 - 2 \\tilde{x}^T \\tilde{y} + 1) \\\\\n","& = 1 - \\mbox{SimilitudCoseno}(x,y).\n","\\end{split}\n","\\end{equation}\n","\n","Podemos calcular la similitud del coseno entre los vectores TFIDF en nuestro corpus de la siguiente manera."]},{"cell_type":"code","metadata":{"id":"aek7l_m8gykN"},"source":["import numpy as np\n","\n","# Normalizamos los documentos\n","X_tfidf_norm = dense / np.linalg.norm(dense, axis=1)[:,None]\n","# Calculamos la similitud del coseno a partir de la última fórmula.\n","M = X_tfidf_norm @ X_tfidf_norm.T\n","print(M)"],"execution_count":null,"outputs":[]}]}